import os
import sys
import time
import re
import tempfile
import difflib
import shutil
import mx.DateTime
import numpy as np
from itertools import count, izip
from Ska.Shell import bash_shell, bash
import Ska.DBI
import Ska.Table
from Chandra.Time import DateTime
from functools import partial
from shutil import copy

#from Ska.Numpy import pprint

import update_load_seg_db

err = sys.stderr
MP_DIR = '/data/mpcrit1/mplogs/'
SKA = os.environ['SKA']
verbose = True
DBI = 'sqlite'

def pprint(recarray, fmt=None, cols=None, out=sys.stdout):
    """
    Print a nicely-formatted version of ``recarray`` to ``out`` file-like object.
    If ``fmt`` is provided it should be a dict of ``colname:fmt_spec`` pairs where
    ``fmt_spec`` is a format specifier (e.g. '%5.2f').

    :param recarray: input record array
    :param fmt: dict of format specifiers (optional)
    :param cols:
    :param out: output file-like object

    :rtype: None
    """

    # Define a dict of pretty-print functions for each column in fmt
    if fmt is None:
        pprint = {}
    else:
        pprint = dict((colname, lambda x: fmt[colname] % x) for colname in fmt)

    colnames = cols
    if colnames == None:
        colnames = recarray.dtype.names

    # Pretty-print all columns and turn into another recarray made of strings
    str_recarray = []
    for row in recarray:
        str_recarray.append([pprint.get(x, str)(row[x]) for x in colnames])
    str_recarray = np.rec.fromrecords(str_recarray, names=colnames)

    # Parse the descr fields of str_recarray recarray to get field width
    colfmt = {}
    for descr in str_recarray.dtype.descr:
        colname, coldescr = descr
        collen = max(int(re.search(r'\d+', coldescr).group()), len(colname))
        colfmt[colname] = '%-' + str(collen) + 's'
    
    # Finally print everything to out
    print >>out, ' '.join(colfmt[x] % x for x in colnames)
    for row in str_recarray:
        print >>out, ' '.join(colfmt[x] % row[x] for x in colnames)


def find_mp_files(loads):
    """
    Find the MP directories that correspond to the list of loads supplied.
    Uses flight/sybase.aca database tables of the parsed files.

    :param loads: list or recarray of load segments
    :rtype: list of directories
    """
    

    db = Ska.DBI.DBI(dbi='sybase', user='aca_read', database='aca', numpy=True)
    # get the modification times of the likely directories
    sumfile_modtimes = []
    for load in loads:
        mx_date = DateTime(load['TStart (GMT)']).mxDateTime
        cl = load['LOADSEG.NAME']
        replan_query = """select tp.sumfile_modtime as sumfile_modtime, *
            from tl_processing as tp, tl_built_loads as tb
                         where tp.file = tb.file
                         and tp.sumfile_modtime = tb.sumfile_modtime
                         and tb.year = %d and tb.load_segment <= "%s"
                         and replan = 0
                         order by tp.sumfile_modtime desc
                         """ % ( mx_date.year, cl )
        replan_match = db.fetchone(replan_query)
        if replan_match:
            sumfile_modtimes.append(replan_match['sumfile_modtime'])
        max_query = """select tp.sumfile_modtime as sumfile_modtime, *
                         from tl_processing as tp, tl_built_loads as tb
                         where tp.file = tb.file
                         and tp.sumfile_modtime = tb.sumfile_modtime
                         and tb.year = %d and tb.load_segment = "%s"
                         order by tp.sumfile_modtime desc
                         """ % ( mx_date.year, cl )
        time_match = db.fetchone(max_query)
        if time_match:
            sumfile_modtimes.append(time_match['sumfile_modtime'])

    # and then fetch the range of products built during this time
    start = min(sumfile_modtimes)
    stop = max(sumfile_modtimes)
    tstart = DateTime( start, format='unix').date
    tstop = DateTime( stop, format='unix').date
    #    if verbose:
    err.write("Fetching mp dirs from %s to %s \n" % (
        tstart, tstop))
    match_query = """select * from tl_processing
                     where sumfile_modtime >= %s
                     and sumfile_modtime <= %s """ % ( start, stop)
    matches = db.fetchall(match_query)
    match_files = matches.dir
    return match_files


def text_compare( testfile, fidfile, outdir, label):
    """
    Diff two files and write html diff to directory.

    :param testfile: file generated by test
    :param fidfile: fiducial file for comparison
    :param outdir: directory for html diff if any
    :param label: label for html diff
    :rtype: boolean, True if files match

    """

    err.write("Comparing %s and %s \n" % (testfile, fidfile))

    # test lines
    test_lines = open(testfile, 'r').readlines() 

    # retrieve fiducial data
    if os.path.exists(fidfile):
        fiducial_lines = open(fidfile, 'r').readlines()
    else:
        fiducial_lines = ''

    # compare
    differ = difflib.context_diff( fiducial_lines, test_lines )
    difflines = [ line for line in differ ]

    if len(difflines):
        err.write("Error on %s, diff in %s \n" % (testfile,
                                                  os.path.join(outdir, 'diff_%s.html' % label)))
        diff_out = open(os.path.join(outdir, 'diff_%s.html' % label), 'w')
        htmldiff = difflib.HtmlDiff()
        htmldiff._styles = htmldiff._styles.replace('Courier', 'monospace')
        diff = htmldiff.make_file( fiducial_lines, test_lines, context=True )
        diff_out.writelines(diff)
        return False
    return True
        


class Scenario(object):
    """
    Test scenario object to hold database and directory setup for a test.
    """

    def __init__(self, outdir, dbi=DBI):
        self.outdir = outdir
        self.dbi = dbi
        self.db_initialized = False
        self.load_rdb = None
        self.mp_dir = None
        self.load_dir = None
        if os.path.exists(outdir):
            shutil.rmtree(outdir)
            err.write("Pre-cleaned by deleting dir %s\n" % outdir)
        os.makedirs(outdir)
        self.dbfile = os.path.join(outdir, 'test.db3')

    def data_setup(self):
        """
        Read a load segment rdb file (from self.load_rdb)
        Figure out which MP directories will be needed
        Make a link tree to those in the output directory (self.outdir)
        Define self.mp_dir and self.load_dir in self.outdir
        Copy over the rdb to another directory in that output directory

        """

        loads = Ska.Table.read_ascii_table(self.load_rdb, datastart=3)
        outdir = self.outdir
        # cheat, and use the real db to get the smallest range that contains
        # all the MP dirs that are listed in the rdb file in the LOADSEG.LOAD_NAME
        # field
        match_files = find_mp_files(loads)

        # gin up a directory tree in outdir
        for m in match_files:
            dirmatch = re.search('(\d{4})\/(\w{7})/(ofls\w)', m)
            if dirmatch:
                year = dirmatch.group(1)
                week = dirmatch.group(2)
                rev = dirmatch.group(3)
                if not os.path.exists(os.path.join(outdir, 'mp', year, week)):
                    os.makedirs( os.path.join( outdir, 'mp', year, week ))
                oldname = os.path.join('/data/mpcrit1/mplogs/', year, week, rev)
                newname = os.path.join( outdir, 'mp', year, week, rev )
                if not os.path.exists(newname):
                    os.symlink(oldname, newname)
                    if verbose:
                        err.write("Linking %s -> %s \n" % (oldname, newname ))

        # copy over load file
        if not os.path.exists(os.path.join(outdir, 'loads')):
            os.mkdir(os.path.join(outdir, 'loads'))
        copy( self.load_rdb, os.path.join(outdir, 'loads'))
        self.load_dir = os.path.join(outdir, 'loads')
        self.mp_dir = os.path.join(outdir, 'mp')
        return

    def db_handle(self):
        if hasattr(self, 'dbh'):
            return self.dbh
        if self.dbi == 'sybase':
            self.dbh = Ska.DBI.DBI(dbi='sybase', server='sybase',
                                   user='aca_test', database='aca_tstdb')
        else:
            self.dbh = Ska.DBI.DBI(dbi='sqlite', server=self.dbfile)
        return self.dbh

    def db_cmd_str(self):
        if hasattr(self, 'db_str'):
            return self.db_str
        if self.dbi == 'sybase':
            self.db_str = " --dbi sybase --server sybase --user aca_test --database aca_tstdb "
        else:
            self.db_str = ' --server %s ' % self.dbfile
        return self.db_str

    def db_wipe(self):
        self.db_setup(wipe=True)

    def cleanup(self):
        self.db_wipe()
        self.dbh.conn.close()
        try:
            shutil.rmtree( self.outdir )
            err.write("Deleted output directory %s\n" % self.outdir )
        except OSError:
            err.write("Passed all tests, but failed to delete dir %s\n" % self.outdir)


    def db_setup(self, wipe=False, tstart=None, tstop=None):
        """ 
        Initialize tables or delete them

        Use the make_new_tables script to initialize new tables and copy over rows from the 
        "real" sybase tables as needed.

        :param tstart: start of range to copy from sybase
        :param tstart: stop of range to copy from sybase
        :param wipe: just delete the tables
        """
        make_new_tables = os.path.join('./make_new_tables.py')

        if wipe:
            make_new_tables += " --wipe "

        testdb = self.db_handle()
        db_str = self.db_cmd_str()

        cmd = "%s %s" % (make_new_tables, db_str)

        if tstart:
            cmd += " --tstart %s " % DateTime(tstart).date
        if tstop:
            cmd += " --tstop %s " % DateTime(tstop).date

        err.write("%s \n" % cmd)
        bash(cmd)

        self.db_initialized = True
        

    def populate_states( self,
                         nonload_cmd_file=None, verbose=False ):
        """
        From the available directories and load segment file
        Populate a load segments table and a timelines table
        Insert the nonload commands
        Insert a handful of states from the real database to seed the commands states
        Build states

        :param nonload_cmd_file: a nonload_cmd_file for the test
        :param verbose: verbosity True/Fals

        """

        outdir = self.outdir
        mp_dir = self.mp_dir
        load_seg_dir = self.load_dir
        db_str = self.db_cmd_str()
        testdb = self.db_handle()

        parse_cmd = os.path.join('./parse_cmd_load_gen.pl')
        parse_cmd_str = ( '%s %s --touch_file %s --mp_dir %s ' %
                          ( parse_cmd, db_str, os.path.join( outdir, 'clg_touchfile'), mp_dir))

        err.write(parse_cmd_str + "\n")
        bash(parse_cmd_str)

        update_load_seg = os.path.join('./update_load_seg_db.py')
        load_seg_cmd_str = ( "%s %s --test --loadseg_rdb_dir '%s'" %
                                ( update_load_seg, db_str, load_seg_dir ))
        err.write(load_seg_cmd_str + "\n")
        load_seg_output = bash(load_seg_cmd_str)

        # update_cmd_states backs up to the first NPNT before the given tstart...
        # backing up doesn't work for this, because we don't have timelines to make 
        # states before the working tstart... 
        # so copy over states from the real db, starting at the start of this
        # timelines timerange until we hit first NPNT state
        # and use the time just after that state as the start of update_cmd_states
        first_timeline = testdb.fetchone("select datestart from timelines")

        acadb = Ska.DBI.DBI(dbi='sybase', user='aca_read', database='aca',
                            numpy=True, verbose=verbose)
        es_query = """select * from cmd_states
                      where datestop > '%s'
                      order by datestart asc""" % first_timeline['datestart'] 
        db_states = acadb.fetch(es_query)

        from itertools import izip, count
        for state, scount in izip(acadb.fetch(es_query), count()):
            testdb.execute("delete from cmd_states where datestart = '%s'"
                           % state['datestart'])
            testdb.insert( state, 'cmd_states' )
            if state['pcad_mode'] == 'NPNT':
                break

        err.write("Copied %d states from flight table from %s to %s \n" % (
            scount,
            first_timeline['datestart'],
            state['datestop']))


        cmd_state_mxstart = ( DateTime( state['datestop'] ).mxDateTime
                              + mx.DateTime.TimeDelta( seconds=1) ) 
        cmd_state_datestart = DateTime(cmd_state_mxstart).date

        #fix_timelines = os.path.join(os.environ['SKA'], 'share', 'timelines', 'fix_timelines.py')
        #bash("%s --server %s " % (fix_timelines, dbfilename ))

        if nonload_cmd_file is None:
            nonload_cmd_file = os.path.join(os.environ['SKA'], 'share',
                                            'cmd_states', 'nonload_cmds_archive.py')

        err.write("%s %s \n" % (nonload_cmd_file, db_str))
        bash("%s %s " % (nonload_cmd_file, db_str ))

        update_cmd_states = os.path.join(os.environ['SKA'], 'share',
                                         'cmd_states', 'update_cmd_states.py')
        err.write("Running %s %s --datestart '%s' --mp_dir %s \n" %
                  (update_cmd_states, db_str, cmd_state_datestart, mp_dir))              
        bash( "%s %s --datestart '%s' --mp_dir %s" %
              (update_cmd_states, db_str, cmd_state_datestart, mp_dir))

        err.write("Updated Test States in %s\n" % outdir )
        return

    def output_text_files(self, prefix=''):

        load_rdb = self.load_rdb
        outdir = self.outdir
        dbh = self.db_handle()
        
        loads = Ska.Table.read_ascii_table(load_rdb, datastart=3)
        datestart = loads[0]['TStart (GMT)']
        datestop = loads[-1]['TStop (GMT)']

        timelines = dbh.fetchall("""select * from timelines
                                    where datestart >= '%s'
                                    and datestop <= '%s'
                                    order by datestart""" % (datestart, datestop))


        tfile = os.path.join( outdir,  prefix+'test_timelines.dat')
        pprint(timelines, cols=['datestart','datestop','dir'], out=open(tfile, 'w'))

        test_states = dbh.fetchall("""select * from cmd_states
                                     where datestart >= '%s'
                                     and datestart <= '%s'
                                     order by datestart""" % (datestart, datestop ))

        fmt = {'power': '%.1f',
               'pitch': '%.2f',
               'tstart': '%.2f',
               'tstop': '%.2f',
               'ra': '%.2f',
               'dec' : '%.2f',
               'roll' : '%.2f',
               'q1' : '%.2f',
               'q2' : '%.2f',
               'q3' : '%.2f',
               'q4' : '%.2f',
               }

        newcols = sorted(list(test_states.dtype.names))
        newcols = [ x for x in newcols if (x != 'tstart') & (x != 'tstop')]
        newstates = np.rec.fromarrays([test_states[x] for x in newcols], names=newcols)

        sfile = os.path.join( outdir, prefix+'test_states.dat')
        pprint(newstates, fmt=fmt, out=open(sfile, 'w'))


        cmds = dbh.fetchall("""select * from cmds
                              where date >= '%s'
                              and date <= '%s'
                              and timeline_id is NULL
                              order by date""" % (datestart, datestop ))

        err.write("%s %s\n" % (datestart, datestop))
        try:
            cmds = cmds.tolist()
        except AttributeError:
            pass

        for tl in timelines:
            tl_cmds = dbh.fetchall("""select * from cmds
                                     where timeline_id = %d
                                     and date >= '%s'
                                     and date <= '%s'
                                     order by date""" % (tl['id'], tl['datestart'], tl['datestop']))
            if len(tl_cmds):
                colnames = tl_cmds.dtype.names
                cmds.extend(tl_cmds.tolist())
        cmds = sorted(cmds, key=lambda x: x[3])
        cmds = np.rec.fromrecords( cmds, names=colnames)

        cfile =  os.path.join( outdir, prefix+'test_cmds.dat')        
        pprint(cmds, out=open(cfile, 'w'))

        
        self.text_files =  { 'timelines': tfile,
                             'states': sfile,
                             'cmds': cfile }

        return self.text_files





def check_load(load_rdb, state_file, expect_match=True):
    """
    Setup testing db for a list of load segments
    Write out the "states" created.
    Diff against fiducial list of states

    :param load_rdb: load segment rdb file
    :param state_file: file with fiducial states
    """

    # make new states from load segments
    s = make_states(load_rdb)
    text_files = s.output_text_files()
    for etype in ['states',]:
        match = text_compare(text_files[etype], state_file, s.outdir, etype)
        if match:
            assert True
        else:
            assert False
    s.cleanup()


def make_states(load_rdb, dbi=DBI, outdir=None):
    """
    Setup db and states in a temp directory for quick nose testing

    :param load_rdb: file with load segments
    :rtype: (outdir, dbfilename)
    """
    if not outdir:
        outdir = re.sub(r'\.rdb$', '', load_rdb)
    s = Scenario(outdir, dbi=dbi)
    s.load_rdb = load_rdb
    s.data_setup()
    s.db_setup()
    s.populate_states()

    return s




### actual tests.
def test_loads():
    """
    Build testing states and test against fiducial data

    Return testing generators for each list of loads to be checked
    """

    # load segment files and states to test against
    good = [
        dict(loads='t/2008:048:08:07:00.000.rdb',
                  states='t/2008:048:08:07:00.000.dat'),
             dict(loads='t/2010:023:15:15:00.000.rdb',
                  states='t/2010:023:15:15:00.000.dat'),
             dict(loads='t/2009:164:04:11:15.022.rdb',
                  states='t/2009:164:04:11:15.022.dat'),
             dict(loads='t/2009:193:20:30:02.056.rdb',
                  states='t/2009:193:20:30:02.056.dat'),
             dict(loads='t/2009:214:22:44:19.592.rdb',
                  states='t/2009:214:22:44:19.592.dat'),
             dict(loads='t/july_fixed.rdb',
                  states='t/july_fixed.dat'),
             dict(loads='t/2009:248:12:39:44.351.rdb',
                  states='t/2009:248:12:39:44.351.dat'),
             dict(loads='t/2009:274:22:25:44.351.rdb',
                  states='t/2009:274:22:25:44.351.dat'),
             dict(loads='t/cut_cl110_1409.rdb',
                  states='t/cut_cl110_1409.dat'),
             dict(loads='t/cut_cl304_0504.rdb',
                  states='t/cut_cl304_0504.dat'),
             dict(loads='t/cut_cl352_1208.rdb',
                  states='t/cut_cl352_1208.dat'),
        ]

    for ftest in good:
        load_rdb = ftest['loads']
        state_file = ftest['states']
        err.write("Checking %s\n" % load_rdb )
        f = partial( check_load, load_rdb, state_file, True)
        f.description = "Confirm %s matches %s states" % (load_rdb, state_file)
        yield(f,)



    # *INCORRECT* fiducial states to check failure
    #bad = [dict(loads='t/july_broken.rdb',
    #            states='t/july_broken.dat')]
    #
    # I don't have a good way to make up broken states with hetg/letg
    # columns handy, so disabling this test for timelines 0.05
    bad = []

    for ftest in bad:
        load_rdb = ftest['loads']
        state_file = ftest['states']
        err.write("Checking %s\n" % load_rdb )
        f = partial( check_loads, load_rdb, state_file, False)
        f.description = "%s unexpectedly matches %s states" % (load_rdb, state_file)
        yield(f,)





def test_get_built_load():

    dbh = Ska.DBI.DBI(dbi='sybase', user='aca_read', database='aca', numpy=True, verbose=False)

    good = [dict(load={'datestart': '2010:052:01:59:26.450',
                         'datestop': '2010:052:12:20:06.101',
                         'fixed_by_hand': 0,
                         'id': 383104832,
                         'load_scs': 128,
                         'load_segment': 'CL052:0101',
                         'year': 2010},
                 built_load={'file': 'C044_2301.sum',
                               'first_cmd_time': '2010:052:01:59:26.450',
                               'last_cmd_time': '2010:052:12:20:06.101',
                               'load_scs': 128,
                               'load_segment': 'CL052:0101',
                               'sumfile_modtime': 1266030114.0,
                               'year': 2010})]
    for ftest in good:
        load = ftest['load']
        built_load = ftest['built_load']
        err.write("Checking get_built_loads\n")
        assert built_load == update_load_seg_db.get_built_load( load, dbh)


def test_get_processing():

    dbh = Ska.DBI.DBI(dbi='sybase', user='aca_read', database='aca', numpy=True, verbose=False)
 
    good = [dict(built_load={'file': 'C044_2301.sum',
                               'first_cmd_time': '2010:052:01:59:26.450',
                               'last_cmd_time': '2010:052:12:20:06.101',
                               'load_scs': 128,
                               'load_segment': 'CL052:0101',
                               'sumfile_modtime': 1266030114.0,
                               'year': 2010},
                 proc={'bcf_cmd_count': None,
                         'continuity_cmds': None,
                         'dir': '/2010/FEB1310/oflsb/',
                         'execution_tstart': '2010:044:03:01:27.000',
                         'file': 'C044_2301.sum',
                         'planning_tstart': '2010:044:23:00:00.000',
                         'planning_tstop': '2010:052:12:23:00.000',
                         'processing_tstart': '2010:044:23:00:00.000',
                         'processing_tstop': '2010:052:12:23:00.000',
                         'replan': 0,
                         'replan_cmds': None,
                         'sumfile_modtime': 1266030114.0,
                         'year': 2010})]

    for ftest in good:
        built_load = ftest['built_load']
        proc = ftest['proc']
        err.write("Checking get_processing \n")
        assert proc == update_load_seg_db.get_processing( built_load, dbh)

def test_weeks_for_load():

    dbh = Ska.DBI.DBI(dbi='sybase', user='aca_read', database='aca', numpy=True, verbose=False)

    good = [dict(load= {'datestart': '2010:052:01:59:26.450',
                          'datestop': '2010:052:12:20:06.101',
                          'fixed_by_hand': 0,
                          'id': 383104832,
                          'load_scs': 128,
                          'load_segment': 'CL052:0101',
                          'year': 2010},
                 new_timelines= [{'datestart': '2010:052:01:59:26.450',
                                    'datestop': '2010:052:12:20:06.101',
                                    'dir': '/2010/FEB1310/oflsb/',
                                    'incomplete': 0,
                                    'load_segment_id': 383104832,
                                    'replan': 0}])]

    for ftest in good:
        load = ftest['load']
        new_timelines=ftest['new_timelines']
        err.write("Checking weeks_for_load \n" )
        assert new_timelines == update_load_seg_db.weeks_for_load( load, dbh)


def test_nsm_2010(outdir='t/nsm_2010', cmd_state_ska=SKA):

    # Simulate timelines and cmd_states around day 150 NSM

    err.write("Running nsm 2010 simulation \n" )

    s = Scenario(outdir)
    s.db_setup()
    db_str = s.db_cmd_str()

    # use a clone of the load_segments "time machine"
    tm = 'iFOT_time_machine'
    repo_dir = '/proj/sot/ska/data/arc/%s/' % tm
    if not os.path.exists(tm):
        c_output = bash_shell("hg clone %s" % repo_dir)
    load_rdb = os.path.join(tm, 'load_segment.rdb')
    
    # make a local copy of nonload_cmds_archive, modified in test directory by interrupt
    shutil.copyfile('t/nsm_nonload_cmds_archive.py', '%s/nonload_cmds_archive.py' % outdir)
    bash_shell("chmod 755 %s/nonload_cmds_archive.py" % outdir)

    # when to begin simulation
    start_time = mx.DateTime.Date(2010,5,30,2,0,0)
    # load interrupt time
    int_time = mx.DateTime.Date(2010,5,30,3,0,0)
    # hours between simulated cron task to update timelines
    step = 12

    for hour_step in np.arange( 0, 72, step):
        ifot_time = start_time + mx.DateTime.DateTimeDelta(0,hour_step)
        # mercurial python stuff doesn't seem to work.
        # grab version of iFOT_time_machine just before simulated date
        
        os.chdir(tm)
        u_output = bash_shell("""hg update --date "%s%s" """ % (
            '<', ifot_time.strftime()))
        os.chdir("..")

        # and copy that load_segment file to testing/working directory
        shutil.copyfile(load_rdb, '%s/%s_loads.rdb' % (outdir, DateTime(ifot_time).date))

        s.load_rdb = load_rdb
        s.data_setup()


        # run the interrupt commanding before running the rest of the commands
        # if the current time is the first simulated cron pass after the
        # actual insertion of the interrupt commands
        dtime = ifot_time - int_time
        if dtime.hours >= 0 and dtime.hours < step:
            err.write( "Performing interrupt \n")
            nsm_cmd = ("%s/share/cmd_states/add_nonload_cmds.py " % cmd_state_ska
                       + " --cmd-set nsm "
                       + " --date '%s'" % DateTime(int_time).date
                       + " --interrupt "
                       + " --archive-file %s/nonload_cmds_archive.py " % outdir
                       + db_str)
            err.write(nsm_cmd + "\n")
            bash_shell(nsm_cmd)

            scs_cmd = ("%s/share/cmd_states/add_nonload_cmds.py " % cmd_state_ska
                       + " --cmd-set scs107 "
                       + " --date '%s'" % DateTime(int_time).date
                       + " --interrupt "
                       + " --archive-file %s/nonload_cmds_archive.py " % outdir
                       + db_str)
            err.write(scs_cmd + "\n")
            bash_shell(scs_cmd)

        prefix = DateTime(ifot_time).date + '_'
        # run the rest of the cron task pieces: parse_cmd_load_gen.pl,
        # update_load_seg_db.py, update_cmd_states.py

        s.populate_states(nonload_cmd_file="%s/nonload_cmds_archive.py" % outdir)
        text_files = s.output_text_files(prefix=prefix)
        for etype in ['states','timelines']:
            fidfile = os.path.join('t', "%sfid_%s.dat" % (prefix, etype))
            match = text_compare(text_files[etype], fidfile, s.outdir, etype)
            if match:
                assert True
            else:
                assert False
        
    s.cleanup()




def test_sosa_scs107(outdir='t/sosa_scs107', cmd_state_ska=os.environ['SKA']):
        
    err.write("Running sosa scs 107 simulation \n" )

    s = Scenario(outdir)
    s.db_setup()
    db_str = s.db_cmd_str()

    # make a local copy of nonload_cmds_archive, (will be modified in test directory by interrupt)
    shutil.copyfile('t/sosa_nonload_cmds_archive.py', '%s/nonload_cmds_archive.py' % outdir)
    bash_shell("chmod 755 %s/nonload_cmds_archive.py" % outdir)

    load_dir = os.path.join(outdir, 'loads')
    os.makedirs(load_dir)
    shutil.copy('t/sosa.rdb', load_dir)
    s.load_rdb = os.path.join(load_dir, 'sosa.rdb')
    s.load_dir = os.path.join(outdir, 'loads')
    s.mp_dir = os.path.join(outdir, 'mp') 

    shutil.copytree('t/sosa_mp', os.path.join(outdir, 'mp'))
    mp_dir = os.path.join(outdir, 'mp')
    
    prefix = 'sosa_pre_'
    s.populate_states(nonload_cmd_file="%s/nonload_cmds_archive.py" % outdir)
    text_files = s.output_text_files(prefix=prefix)
    for etype in ['states','timelines']:
        fidfile = os.path.join('t', "%sfid_%s.dat" % (prefix, etype))
        match = text_compare(text_files[etype], fidfile, s.outdir, etype)
        if match:
            assert True
        else:
            assert False


    #shutil.copyfile( dbfilename, os.path.join(outdir, 'db_pre_interrupt.db3'))
    

    # a time for the interrupt (chosen in the middle of a segment)
    int_time = '2010:278:20:00:00.000'

    # run the interrupt commanding before running the rest of the commands
    # if the current time is the first simulated cron pass after the
    # actual insertion of the interrupt commands
    #print "Performing SCS107 interrupt"
    scs_cmd = ("%s/share/cmd_states/add_nonload_cmds.py " % cmd_state_ska
               + " --cmd-set scs107 "
               + " --date '%s'" % DateTime(int_time).date
               + " --interrupt_observing "
               + " --archive-file %s/nonload_cmds_archive.py " % outdir
               + db_str)
    err.write(scs_cmd + "\n")
    bash_shell(scs_cmd)


    prefix = 'sosa_post_'
    s.populate_states(nonload_cmd_file="%s/nonload_cmds_archive.py" % outdir)
    text_files = s.output_text_files(prefix=prefix)
    for etype in ['states','timelines']:
        fidfile = os.path.join('t', "%sfid_%s.dat" % (prefix, etype))
        match = text_compare(text_files[etype], fidfile, s.outdir, etype)
        if match:
            assert True
        else:
            assert False
    s.cleanup()


def test_sosa_nsm(outdir='t/sosa_nsm', cmd_state_ska=os.environ['SKA']):
        
    err.write("Running sosa nsm simulation \n" )

    s = Scenario(outdir)
    s.db_setup()
    db_str = s.db_cmd_str()

    # make a local copy of nonload_cmds_archive, (will be modified in test directory by interrupt)
    shutil.copyfile('t/sosa_nonload_cmds_archive.py', '%s/nonload_cmds_archive.py' % outdir)
    bash_shell("chmod 755 %s/nonload_cmds_archive.py" % outdir)

    load_dir = os.path.join(outdir, 'loads')
    os.makedirs(load_dir)
    shutil.copy('t/sosa.rdb', load_dir)
    s.load_rdb = os.path.join(load_dir, 'sosa.rdb')
    s.load_dir = os.path.join(outdir, 'loads')
    s.mp_dir = os.path.join(outdir, 'mp') 

    shutil.copytree('t/sosa_mp', os.path.join(outdir, 'mp'))
    mp_dir = os.path.join(outdir, 'mp')
    
    prefix = 'sosa_nsmpre_'
    s.populate_states(nonload_cmd_file="%s/nonload_cmds_archive.py" % outdir)
    text_files = s.output_text_files(prefix=prefix)
    for etype in ['states','timelines']:
        fidfile = os.path.join('t', "%sfid_%s.dat" % (prefix, etype))
        match = text_compare(text_files[etype], fidfile, s.outdir, etype)
        if match:
            assert True
        else:
            assert False


    #shutil.copyfile( dbfilename, os.path.join(outdir, 'db_pre_interrupt.db3'))
    

    # a time for the interrupt (chosen in the middle of a segment)
    int_time = '2010:278:20:00:00.000'

    # run the interrupt commanding before running the rest of the commands
    # if the current time is the first simulated cron pass after the
    # actual insertion of the interrupt commands
    nsm_cmd = ("%s/share/cmd_states/add_nonload_cmds.py " % cmd_state_ska
               + " --cmd-set nsm "
               + " --date '%s'" % DateTime(int_time).date
               + " --interrupt "
               + " --archive-file %s/nonload_cmds_archive.py " % outdir
               + db_str)
    err.write(nsm_cmd + "\n")
    bash_shell(nsm_cmd)

    scs_cmd = ("%s/share/cmd_states/add_nonload_cmds.py " % cmd_state_ska
               + " --cmd-set scs107 "
               + " --date '%s'" % DateTime(int_time).date
               + " --interrupt "
               + " --archive-file %s/nonload_cmds_archive.py " % outdir
               + db_str)
    err.write(scs_cmd + "\n")
    bash_shell(scs_cmd)


    prefix = 'sosa_nsmpost_'
    s.populate_states(nonload_cmd_file="%s/nonload_cmds_archive.py" % outdir)
    text_files = s.output_text_files(prefix=prefix)
    for etype in ['states','timelines']:
        fidfile = os.path.join('t', "%sfid_%s.dat" % (prefix, etype))
        match = text_compare(text_files[etype], fidfile, s.outdir, etype)
        if match:
            assert True
        else:
            assert False

    s.cleanup()



def run_model(opt, db):
    """
    Run the psmc twodof model for given states 

    :param opt: cmd line options opt.load_rdb, opt.verbose, opt.outdir
    :rtype: dict of just about everything
    """

    import Ska.Table

    loads = Ska.Table.read_ascii_table(opt.load_rdb, datastart=3)
    datestart = loads[0]['TStart (GMT)']
    datestop = loads[-1]['TStop (GMT)']


    # Add psmc dirs
    psmc_dir = os.path.join(os.environ['SKA'], 'share', 'psmc')
    sys.path.append(psmc_dir)
    import Chandra.cmd_states as cmd_states
    import Ska.TelemArchive.fetch
    import numpy as np
    states = db.fetchall("""select * from cmd_states
                            where datestart >= '%s'
                            and datestart <= '%s'
                            order by datestart""" % (datestart, datestop ))
    colspecs = ['1pdeaat', '1pin1at',
                'tscpos', 'aosares1',
                '1de28avo', '1deicacu',
                '1dp28avo', '1dpicacu',
                '1dp28bvo', '1dpicbcu']

    print "Fetching from %s to %s" % ( states[0]['datestart'], states[-2]['datestart'])
    colnames, vals = Ska.TelemArchive.fetch.fetch( start=states[0]['datestart'],
                                                   stop=states[-2]['datestart'],
                                                   dt=32.8,
                                                   colspecs=colspecs )

    tlm = np.rec.fromrecords( vals, names=colnames )
    import psmc_check
    plots_validation = psmc_check.make_validation_plots( opt, tlm, db )
    valid_viols = psmc_check.make_validation_viols(plots_validation)
    import characteristics
    MSID = dict(dea='1PDEAAT', pin='1PIN1AT')
    YELLOW = dict(dea=characteristics.T_dea_yellow, pin=characteristics.T_pin_yellow)
    MARGIN = dict(dea=characteristics.T_dea_margin, pin=characteristics.T_pin_margin)
    proc = dict(run_user=os.environ['USER'],
                run_time=time.ctime(),
                errors=[],
                dea_limit=YELLOW['dea'] - MARGIN['dea'],
                pin_limit=YELLOW['pin'] - MARGIN['pin'],
                )

    pred = dict(plots=None, viols=None, times=None, states=None, temps=None)  
    psmc_check.write_index_rst(opt, proc, plots_validation, valid_viols=valid_viols, 
                               plots=pred['plots'], viols=pred['viols'])
    psmc_check.rst_to_html(opt, proc)
    
    return dict(opt=opt, states=pred['states'], times=pred['times'],
                temps=pred['temps'], plots=pred['plots'],
                viols=pred['viols'], proc=proc, 
                plots_validation=plots_validation)

